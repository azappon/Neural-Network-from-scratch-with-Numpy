{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data into numpy array to apply basic linear algebra operations more easily\n",
    "mnist = pd.read_csv('/Users/lorenzoriccio/Downloads/digit-recognizer/mnist_train.csv')\n",
    "mnist =  np.array(mnist)\n",
    "np.random.shuffle(mnist)\n",
    "mnist = mnist\n",
    "\n",
    "tot_images = len(mnist)\n",
    "\n",
    "# split the dataset into training and testing sets 80%/20%\n",
    "train_mnist, test_mnist = mnist[:int(tot_images*0.8)].T, mnist[int(tot_images*0.8):].T\n",
    "\n",
    "# split  data into images and lables, normalize the images (0-1, computationally faster)\n",
    "train_imgs, train_lables = train_mnist[1:] / 255, train_mnist[0]\n",
    "test_imgs, test_lables =   test_mnist[1:] / 255, test_mnist[0]\n",
    "\n",
    "n_features, n_images = train_imgs.shape\n",
    "n_classes = m = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to initialize W1, W2 and b1, b2\n",
    "def initialize_parameters(n_classes, n_features):\n",
    "    W1 = np.random.rand(n_classes, n_features) - 0.5\n",
    "    b1 = np.zeros((n_classes, 1))\n",
    "    W2 = np.random.rand(n_classes, n_classes) - 0.5\n",
    "    b2 = np.zeros((n_classes, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "    \n",
    "# define foward probability function \n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    # calculate Z1\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    # calculate A1 using ReLU\n",
    "    A1 = np.maximum(0, Z1)\n",
    "    # calculate Z2\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    # calculate A2 using Softmax\n",
    "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "    return A1, A2\n",
    "\n",
    "\n",
    "# one hot function to convert labels to one hot vectors\n",
    "def one_hot(y, n_classes=10):\n",
    "    one_hot = np.zeros((n_classes, y.shape[0]))\n",
    "    for i in range(y.shape[0]):\n",
    "        one_hot[y[i], i] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# define a backward propagation function\n",
    "def backward(X, Y, A1, A2, W1, W2):\n",
    "    # calculate dZ2\n",
    "    dZ2 = A2 - one_hot(Y)\n",
    "    # calculate dW2\n",
    "    dW2 = np.dot(dZ2, A1.T) / X.shape[1]\n",
    "    # calculate db2\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / X.shape[1]\n",
    "    # calculate dA1\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    # calculate dZ1 using ReLU derivative\n",
    "    dZ1 = dA1 * (A1 > 0)\n",
    "    # calculate dW1\n",
    "    dW1 = np.dot(dZ1, X.T) / X.shape[1]\n",
    "    # calculate db1\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / X.shape[1]\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "# define update parameters function\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    # update W1\n",
    "    W1 = W1 - alpha * dW1\n",
    "    # update b1\n",
    "    b1 = b1 - alpha * db1\n",
    "    # update W2\n",
    "    W2 = W2 - alpha * dW2\n",
    "    # update b2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# define get prediction function\n",
    "def get_prediction(X, W1, b1, W2, b2):\n",
    "    A1, A2 = forward(X, W1, b1, W2, b2)\n",
    "    return np.argmax(A2, axis=0)\n",
    "\n",
    "def get_accuracy(X, Y, W1, b1, W2, b2):\n",
    "    pred = get_prediction(X, W1, b1, W2, b2)\n",
    "    return np.mean(pred == Y)\n",
    "\n",
    "# define train function (gradient descent)\n",
    "def train(X, Y, n_iterations=500, alpha=0.1):\n",
    "    W1, b1, W2, b2 = initialize_parameters(n_classes, n_features)\n",
    "    for i in range(n_iterations):\n",
    "        A1, A2 = forward(X, W1, b1, W2, b2)\n",
    "        dW1, db1, dW2, db2 = backward(X, Y, A1, A2, W1, W2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(\"Accuracy: \", get_accuracy(X, Y, W1, b1, W2, b2))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Accuracy:  0.11029761904761905\n",
      "Iteration:  10\n",
      "Accuracy:  0.250625\n",
      "Iteration:  20\n",
      "Accuracy:  0.39574404761904763\n",
      "Iteration:  30\n",
      "Accuracy:  0.5010416666666667\n",
      "Iteration:  40\n",
      "Accuracy:  0.5894642857142857\n",
      "Iteration:  50\n",
      "Accuracy:  0.6454761904761904\n",
      "Iteration:  60\n",
      "Accuracy:  0.6866369047619048\n",
      "Iteration:  70\n",
      "Accuracy:  0.714672619047619\n",
      "Iteration:  80\n",
      "Accuracy:  0.7383035714285714\n",
      "Iteration:  90\n",
      "Accuracy:  0.7546428571428572\n",
      "Iteration:  100\n",
      "Accuracy:  0.7685416666666667\n",
      "Iteration:  110\n",
      "Accuracy:  0.7798511904761904\n",
      "Iteration:  120\n",
      "Accuracy:  0.7888392857142857\n",
      "Iteration:  130\n",
      "Accuracy:  0.7967261904761904\n",
      "Iteration:  140\n",
      "Accuracy:  0.8035714285714286\n",
      "Iteration:  150\n",
      "Accuracy:  0.8094642857142857\n",
      "Iteration:  160\n",
      "Accuracy:  0.8146428571428571\n",
      "Iteration:  170\n",
      "Accuracy:  0.8193452380952381\n",
      "Iteration:  180\n",
      "Accuracy:  0.8240178571428571\n",
      "Iteration:  190\n",
      "Accuracy:  0.828422619047619\n",
      "Iteration:  200\n",
      "Accuracy:  0.8319940476190476\n",
      "Iteration:  210\n",
      "Accuracy:  0.8353571428571429\n",
      "Iteration:  220\n",
      "Accuracy:  0.8388392857142857\n",
      "Iteration:  230\n",
      "Accuracy:  0.8417559523809524\n",
      "Iteration:  240\n",
      "Accuracy:  0.8448214285714286\n",
      "Iteration:  250\n",
      "Accuracy:  0.8472619047619048\n",
      "Iteration:  260\n",
      "Accuracy:  0.85\n",
      "Iteration:  270\n",
      "Accuracy:  0.8522619047619048\n",
      "Iteration:  280\n",
      "Accuracy:  0.8544345238095238\n",
      "Iteration:  290\n",
      "Accuracy:  0.8563988095238095\n",
      "Iteration:  300\n",
      "Accuracy:  0.858452380952381\n",
      "Iteration:  310\n",
      "Accuracy:  0.8602678571428571\n",
      "Iteration:  320\n",
      "Accuracy:  0.8618154761904762\n",
      "Iteration:  330\n",
      "Accuracy:  0.8636607142857143\n",
      "Iteration:  340\n",
      "Accuracy:  0.8652678571428571\n",
      "Iteration:  350\n",
      "Accuracy:  0.8665773809523809\n",
      "Iteration:  360\n",
      "Accuracy:  0.8680952380952381\n",
      "Iteration:  370\n",
      "Accuracy:  0.8696130952380953\n",
      "Iteration:  380\n",
      "Accuracy:  0.8713095238095238\n",
      "Iteration:  390\n",
      "Accuracy:  0.8723214285714286\n",
      "Iteration:  400\n",
      "Accuracy:  0.8736904761904762\n",
      "Iteration:  410\n",
      "Accuracy:  0.8750892857142857\n",
      "Iteration:  420\n",
      "Accuracy:  0.8761309523809524\n",
      "Iteration:  430\n",
      "Accuracy:  0.8773809523809524\n",
      "Iteration:  440\n",
      "Accuracy:  0.878125\n",
      "Iteration:  450\n",
      "Accuracy:  0.8792559523809523\n",
      "Iteration:  460\n",
      "Accuracy:  0.880625\n",
      "Iteration:  470\n",
      "Accuracy:  0.8813988095238096\n",
      "Iteration:  480\n",
      "Accuracy:  0.8820535714285714\n",
      "Iteration:  490\n",
      "Accuracy:  0.8826488095238095\n",
      "Iteration:  500\n",
      "Accuracy:  0.8833928571428571\n",
      "Iteration:  510\n",
      "Accuracy:  0.8841071428571429\n",
      "Iteration:  520\n",
      "Accuracy:  0.8848511904761904\n",
      "Iteration:  530\n",
      "Accuracy:  0.8854761904761905\n",
      "Iteration:  540\n",
      "Accuracy:  0.8860416666666666\n",
      "Iteration:  550\n",
      "Accuracy:  0.8868452380952381\n",
      "Iteration:  560\n",
      "Accuracy:  0.8875595238095239\n",
      "Iteration:  570\n",
      "Accuracy:  0.8880357142857143\n",
      "Iteration:  580\n",
      "Accuracy:  0.8888392857142857\n",
      "Iteration:  590\n",
      "Accuracy:  0.8892857142857142\n",
      "Iteration:  600\n",
      "Accuracy:  0.8899702380952381\n",
      "Iteration:  610\n",
      "Accuracy:  0.8903571428571428\n",
      "Iteration:  620\n",
      "Accuracy:  0.890922619047619\n",
      "Iteration:  630\n",
      "Accuracy:  0.8914583333333334\n",
      "Iteration:  640\n",
      "Accuracy:  0.8921130952380952\n",
      "Iteration:  650\n",
      "Accuracy:  0.8927380952380952\n",
      "Iteration:  660\n",
      "Accuracy:  0.8932738095238095\n",
      "Iteration:  670\n",
      "Accuracy:  0.8941666666666667\n",
      "Iteration:  680\n",
      "Accuracy:  0.894702380952381\n",
      "Iteration:  690\n",
      "Accuracy:  0.8950595238095238\n",
      "Iteration:  700\n",
      "Accuracy:  0.8952678571428572\n",
      "Iteration:  710\n",
      "Accuracy:  0.8956845238095238\n",
      "Iteration:  720\n",
      "Accuracy:  0.8961309523809524\n",
      "Iteration:  730\n",
      "Accuracy:  0.8963988095238096\n",
      "Iteration:  740\n",
      "Accuracy:  0.8967857142857143\n",
      "Iteration:  750\n",
      "Accuracy:  0.8971428571428571\n",
      "Iteration:  760\n",
      "Accuracy:  0.8976785714285714\n",
      "Iteration:  770\n",
      "Accuracy:  0.8982142857142857\n",
      "Iteration:  780\n",
      "Accuracy:  0.8986904761904762\n",
      "Iteration:  790\n",
      "Accuracy:  0.8990178571428571\n",
      "Iteration:  800\n",
      "Accuracy:  0.8993154761904761\n",
      "Iteration:  810\n",
      "Accuracy:  0.8996428571428572\n",
      "Iteration:  820\n",
      "Accuracy:  0.9001190476190476\n",
      "Iteration:  830\n",
      "Accuracy:  0.900327380952381\n",
      "Iteration:  840\n",
      "Accuracy:  0.9006547619047619\n",
      "Iteration:  850\n",
      "Accuracy:  0.9011607142857143\n",
      "Iteration:  860\n",
      "Accuracy:  0.9011904761904762\n",
      "Iteration:  870\n",
      "Accuracy:  0.9014285714285715\n",
      "Iteration:  880\n",
      "Accuracy:  0.9019345238095238\n",
      "Iteration:  890\n",
      "Accuracy:  0.9022321428571428\n",
      "Iteration:  900\n",
      "Accuracy:  0.9023214285714286\n",
      "Iteration:  910\n",
      "Accuracy:  0.9024702380952381\n",
      "Iteration:  920\n",
      "Accuracy:  0.9027083333333333\n",
      "Iteration:  930\n",
      "Accuracy:  0.9027678571428571\n",
      "Iteration:  940\n",
      "Accuracy:  0.9029761904761905\n",
      "Iteration:  950\n",
      "Accuracy:  0.9032738095238095\n",
      "Iteration:  960\n",
      "Accuracy:  0.9036011904761905\n",
      "Iteration:  970\n",
      "Accuracy:  0.9038988095238095\n",
      "Iteration:  980\n",
      "Accuracy:  0.9043154761904761\n",
      "Iteration:  990\n",
      "Accuracy:  0.9046726190476191\n",
      "Iteration:  1000\n",
      "Accuracy:  0.9048809523809523\n",
      "Iteration:  1010\n",
      "Accuracy:  0.905327380952381\n",
      "Iteration:  1020\n",
      "Accuracy:  0.9056547619047619\n",
      "Iteration:  1030\n",
      "Accuracy:  0.9058630952380953\n",
      "Iteration:  1040\n",
      "Accuracy:  0.9061309523809524\n",
      "Iteration:  1050\n",
      "Accuracy:  0.9064583333333334\n",
      "Iteration:  1060\n",
      "Accuracy:  0.9066369047619047\n",
      "Iteration:  1070\n",
      "Accuracy:  0.9067261904761905\n",
      "Iteration:  1080\n",
      "Accuracy:  0.907172619047619\n",
      "Iteration:  1090\n",
      "Accuracy:  0.9074107142857143\n",
      "Iteration:  1100\n",
      "Accuracy:  0.9075297619047619\n",
      "Iteration:  1110\n",
      "Accuracy:  0.9076785714285714\n",
      "Iteration:  1120\n",
      "Accuracy:  0.9077678571428571\n",
      "Iteration:  1130\n",
      "Accuracy:  0.9078869047619048\n",
      "Iteration:  1140\n",
      "Accuracy:  0.9081547619047619\n",
      "Iteration:  1150\n",
      "Accuracy:  0.9083333333333333\n",
      "Iteration:  1160\n",
      "Accuracy:  0.9084821428571429\n",
      "Iteration:  1170\n",
      "Accuracy:  0.9089880952380952\n",
      "Iteration:  1180\n",
      "Accuracy:  0.9092559523809524\n",
      "Iteration:  1190\n",
      "Accuracy:  0.9095833333333333\n",
      "Iteration:  1200\n",
      "Accuracy:  0.9097619047619048\n",
      "Iteration:  1210\n",
      "Accuracy:  0.9101488095238095\n",
      "Iteration:  1220\n",
      "Accuracy:  0.9104166666666667\n",
      "Iteration:  1230\n",
      "Accuracy:  0.9105952380952381\n",
      "Iteration:  1240\n",
      "Accuracy:  0.9107142857142857\n",
      "Iteration:  1250\n",
      "Accuracy:  0.9108333333333334\n",
      "Iteration:  1260\n",
      "Accuracy:  0.9109226190476191\n",
      "Iteration:  1270\n",
      "Accuracy:  0.9111904761904762\n",
      "Iteration:  1280\n",
      "Accuracy:  0.9114583333333334\n",
      "Iteration:  1290\n",
      "Accuracy:  0.9116071428571428\n",
      "Iteration:  1300\n",
      "Accuracy:  0.9117857142857143\n",
      "Iteration:  1310\n",
      "Accuracy:  0.9120833333333334\n",
      "Iteration:  1320\n",
      "Accuracy:  0.9122619047619047\n",
      "Iteration:  1330\n",
      "Accuracy:  0.9125\n",
      "Iteration:  1340\n",
      "Accuracy:  0.9128273809523809\n",
      "Iteration:  1350\n",
      "Accuracy:  0.9130952380952381\n",
      "Iteration:  1360\n",
      "Accuracy:  0.9132738095238095\n",
      "Iteration:  1370\n",
      "Accuracy:  0.9135119047619048\n",
      "Iteration:  1380\n",
      "Accuracy:  0.9135714285714286\n",
      "Iteration:  1390\n",
      "Accuracy:  0.9139285714285714\n",
      "Iteration:  1400\n",
      "Accuracy:  0.9141071428571429\n",
      "Iteration:  1410\n",
      "Accuracy:  0.9142559523809524\n",
      "Iteration:  1420\n",
      "Accuracy:  0.914375\n",
      "Iteration:  1430\n",
      "Accuracy:  0.9144047619047619\n",
      "Iteration:  1440\n",
      "Accuracy:  0.9145238095238095\n",
      "Iteration:  1450\n",
      "Accuracy:  0.914702380952381\n",
      "Iteration:  1460\n",
      "Accuracy:  0.9147916666666667\n",
      "Iteration:  1470\n",
      "Accuracy:  0.9149702380952381\n",
      "Iteration:  1480\n",
      "Accuracy:  0.9149107142857142\n",
      "Iteration:  1490\n",
      "Accuracy:  0.9150297619047619\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = train(train_imgs, train_lables, n_iterations=1500, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and display!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test set:  0.9016666666666666\n",
      "Model generalizes well!\n",
      "Model is very accurate!\n"
     ]
    }
   ],
   "source": [
    "# obtain predictions on the test set\n",
    "pred = get_prediction(test_imgs, W1, b1, W2, b2)\n",
    "# calculate accuracy\n",
    "acc = get_accuracy(test_imgs, test_lables, W1, b1, W2, b2)\n",
    "print(\"Accuracy in test set: \", acc)\n",
    "if acc > 0.8:\n",
    "    print(\"Model generalizes well!\")\n",
    "else:\n",
    "    print(\"Model does not generalize well!\")\n",
    "if acc > 0.9:\n",
    "    print(\"Model is very accurate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(10, 4))\n",
    "\n",
    "for i in range(5):\n",
    "    r = np.random.randint(0, len(test_imgs))\n",
    "    ax[i].imshow(test_imgs[:, r].reshape(28, 28), cmap='gray')\n",
    "    ax[i].set_title(\"Predicted: {}\".format(pred[r]))\n",
    "    ax[i].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6122458b6ebdaeb06e04e8aa17f2b8e95e321b5775903159a255c98cfe828f5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
